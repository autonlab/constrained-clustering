{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows different visualizations of the algorithms performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fold import computedFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from fold import readFold\n",
    "import matplotlib.pyplot as plt\n",
    "from pmlb import classification_dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open all the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all methods are not computed at the same date, indicates several dates in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDictToFrame(dictionary, keys = []):\n",
    "    \"\"\"\n",
    "        Transfroms a multidimensional dictionary in a dataframe\n",
    "    \"\"\"\n",
    "    df = {}\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):\n",
    "            for k, v in fromDictToFrame(value, keys + [key]).items():\n",
    "                df[k] = v\n",
    "        else:\n",
    "            df[tuple(keys + [key])] = [value]\n",
    "    return df\n",
    "\n",
    "def openFiles(dates, subfolder_detection = False, join = \"inner\"):\n",
    "    \"\"\"\n",
    "        Opens all files at the given dates and restructures it in several dataframes for clarity\n",
    "    \"\"\"\n",
    "    if subfolder_detection:\n",
    "        dates = [os.path.join(date, method) for date in dates for method in os.listdir(os.path.join(config.result, date)) ]\n",
    "        \n",
    "    scores, assignation, constraints, info = [], [], [], {}\n",
    "    for date in dates:\n",
    "        print(\"Opening {}\".format(date))\n",
    "        path = os.path.join(config.result, date)\n",
    "        scoresDate, assignationDate, constraintsDate = {}, {}, {}\n",
    "        \n",
    "        # Open each dataset\n",
    "        for dname in tqdm(os.listdir(path)):\n",
    "            unpick = pickle.load(open(os.path.join(path, dname), 'rb'))\n",
    "\n",
    "            # Open Score\n",
    "            scoreDname = pd.DataFrame.from_dict(fromDictToFrame(unpick[\"Score\"]))\n",
    "            if not scoreDname.empty:\n",
    "                scoresDate[dname] = scoreDname\n",
    "\n",
    "                # Open Assignation\n",
    "                assignationDate[dname] = pd.DataFrame.from_dict(fromDictToFrame(unpick[\"Assignation\"]))\n",
    "\n",
    "                # Open Constraints\n",
    "                constraintsDate[dname] = pd.DataFrame.from_dict(fromDictToFrame(unpick[\"Percentage Constraint\"]))\n",
    "\n",
    "                # Open info\n",
    "                info[dname] = pd.DataFrame.from_dict(fromDictToFrame(readFold(unpick[\"Name\"])))\n",
    "\n",
    "        if len(scoresDate) > 0:\n",
    "            scores.append(pd.concat(scoresDate, axis = \"index\").reset_index(level=[1], drop=True))\n",
    "            assignation.append(pd.concat(assignationDate, axis = \"index\").reset_index(level=[1], drop=True))\n",
    "            constraints.append(pd.concat(constraintsDate, axis = \"index\").reset_index(level=[1], drop=True))\n",
    "        else:\n",
    "            print(\" -> Empty\")\n",
    "            \n",
    "    return pd.concat(scores, join = join, axis = 1),\\\n",
    "        pd.concat(assignation, join = join, axis = 1),\\\n",
    "        pd.concat(constraints, join = join, axis = 1),\\\n",
    "        pd.concat(info, axis = \"index\").reset_index(level=[1], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['24 April 2019 15:28:13'] #05 April 2019 11:59:58 #03 April 2019 15:45:13 #19 April 2019 12:11:09 #24 April 2019 15:28:13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, assignation, constraints, info = openFiles(dates, True)\n",
    "print(\"Open {} result files\".format(len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"FScore\" #FScore #Normalized Mutual Info #Adjusted Rand\n",
    "category = \"test\" # 'all', 'train' or 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes mean of the different iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_score = scores.loc[:, (slice(None), slice(None), category, metric)].copy()\n",
    "folder_score = folder_score.T.reset_index(level=[2, 3], drop=True).T\n",
    "scores_average = scores.groupby(axis=1, level=[1, 2, 3]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the global performances of the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores_average.loc[:, (slice(None), category, metric)].copy()\n",
    "measure = measure.T.reset_index(level=[1, 2], drop=True)\n",
    "measure = measure.loc[measure.median(axis = 1).sort_values(ascending = False).index].T\n",
    "sns.boxplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)\n",
    "plt.savefig('{}_{}.eps'.format(metric, category), format='eps', dpi=1000, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis by method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"Bayesian Optimization SoftKmeans\"\n",
    "metric = \"Adjusted Rand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and display the rank of the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_methods = [\"Bayesian Optimization SoftKmeans\", \"Bayesian Optimization\", \"Bayesian mahalanobis\", \"Bayesian cosine\", \"Cross Validation\", \"Cross Validation SoftKmeans\"]\n",
    "var = [v for v in measure.columns if v not in our_methods] + [method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rank position\n",
    "ranks = np.arange(1, len(var)).tolist()\n",
    "rank = measure[var].rank(axis = 1, method = 'min', ascending = False).astype(int)\n",
    "count = rank.apply(lambda x: x.value_counts()).T\n",
    "count = count.sort_values(by=ranks, ascending = False)[ranks] # Reorder\n",
    "a = count.plot.barh(stacked=True)\n",
    "a.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Number of datasets\")\n",
    "plt.savefig('{}_{}_rank.eps'.format(metric, category), format='eps', dpi=1000, bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "ranks = [1, 2, 3]\n",
    "count = count.sort_values(by=ranks, ascending = False)[ranks] # Reorder\n",
    "a = count.plot.barh(stacked=True)\n",
    "for ir in ranks:\n",
    "    significant = {v: 0 for v in var}\n",
    "    for i, s in folder_score.iterrows():\n",
    "        for best in rank.loc[i].index[rank.loc[i] == ir]:\n",
    "            lower = s[(slice(None), best)].mean() - 1.96 * s[(slice(None), best)].std() / np.sqrt(len(s[(slice(None), best)]))\n",
    "            significant[best] += all([lower > (s[(slice(None),r)].mean() + 1.96 * s[(slice(None),r)].std() / np.sqrt(len(s[(slice(None),r)]))) for r in rank.loc[i].index if ((r != best) and (rank.loc[i][r] >= ir))])\n",
    "    significant = pd.DataFrame.from_dict(significant, orient=\"index\", columns=[\"Significant \"]).loc[count.index]\n",
    "    a = significant.plot.barh(ax = a, color = \"k\", alpha = 0.25, hatch='//', left = count[[c for c in ranks if c < ir]].sum(axis = 1), legend = (ir == ranks[-1]))\n",
    "\n",
    "plt.xlabel(\"Number of datasets\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.savefig('{}_{}_rank_sign.eps'.format(metric, category), format='eps', dpi=1000, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = count.plot.barh(stacked=True)\n",
    "ranks = [2, 3]\n",
    "for ir in ranks:\n",
    "    previous = {v: 0 for v in var}\n",
    "    for i, s in folder_score.iterrows():\n",
    "        for best in rank.loc[i].index[rank.loc[i] == ir]:\n",
    "            upper = s[(slice(None), best)].mean() + 1.96 * s[(slice(None), best)].std() / np.sqrt(len(s[(slice(None), best)]))\n",
    "            previous[best] += all([upper > (s[(slice(None),r)].mean() - 1.96 * s[(slice(None),r)].std() / np.sqrt(len(s[(slice(None),r)]))) for r in rank.loc[i].index if (rank.loc[i][r] == ir - 1)])\n",
    "    previous = pd.DataFrame.from_dict(previous, orient=\"index\", columns=[\"Overlap Previous\"]).loc[count.index]\n",
    "    previous.plot.barh(ax = a, color = \"w\", alpha = 0.25, hatch='//', edgecolor=\"w\", left = count[[c for c in range(1, ir)]].sum(axis = 1), legend = (ir == ranks[-1]))\n",
    "plt.xlabel(\"Number of datasets\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig('{}_{}.eps'.format(metric, category), format='eps', dpi=1000, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versus all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [v for v in var if v != method]\n",
    "number_per_line = 4 \n",
    "cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[\"Number of points\"] = info['Labels', None].apply(lambda x: len(x))\n",
    "info[\"Number of clusters\"] = info[\"N_Classes\", None]\n",
    "number_line = np.ceil(len(var) / number_per_line).astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(number_line, number_per_line, figsize = (4 * number_per_line, 4 * number_line), sharey=True)\n",
    "for i, v in enumerate(var):\n",
    "    a = axes[i % number_line, i // number_line]\n",
    "    a.plot([0, 1], [0, 1], ls = ':')\n",
    "    a = sns.scatterplot(ax = a, x = v, y = method, data = measure, sizes=(10, 200), size=info[\"Number of points\"], hue=info[\"Number of clusters\"], alpha = 0.7, palette = cmap, legend = 'brief' if (v == var[-1]) else False)\n",
    "else:\n",
    "    a.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    for j in range(i + 1, number_line * number_per_line):\n",
    "        axes[j  % number_line, j // number_line].axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('{}_{}_comparison.eps'.format(metric, category), format='eps', dpi=1000, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performances between training and test, it is interesting to notice how our method does not overfit where other methods perform well on training but have bad performances on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores_average.loc[:, (method, slice(None), metric)].copy()\n",
    "measure = measure.T.reset_index(level=[0, 2], drop=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"train\", y=\"test\", sizes=(10, 200), data=measure, \n",
    "                size=info[\"Number of points\"], hue=info[\"Number of clusters\"], palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the percentage of cosntraint respected on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constraint import verification_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageConstraint(gtLabels, label, trainIndices, test = False):\n",
    "    \"\"\"\n",
    "        Compute the percentage of constraint respected\n",
    "        gtLabels -- Ground truth label\n",
    "        label -- Ground truth label\n",
    "        trainIndices -- Indices for train\n",
    "        test -- Compute on test supset if True\n",
    "    \"\"\"\n",
    "    if test:\n",
    "        trainIndices = [i for i in range(len(gtLabels)) if i not in trainIndices]\n",
    "    gtConstraint = 2*np.equal.outer(gtLabels[trainIndices], gtLabels[trainIndices]) - 1\n",
    "\n",
    "    verified, notVerified = verification_constraint(gtConstraint, label[trainIndices])\n",
    "    return  verified / (notVerified + verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure[\"% train constraint respected\"] = pd.DataFrame.from_dict({dname: np.mean([percentageConstraint(info.loc[dname]['Labels', pd.NaT],\n",
    "                                                                    assignation.loc[dname][i, method],\n",
    "                                                                    info.loc[dname]['Train', i, method])\n",
    "                                                                for i in assignation.loc[dname].index.levels[0]])\n",
    "                                                        for dname in measure.index}, orient = 'index')\n",
    "\n",
    "measure[\"% test constraint respected\"] = pd.DataFrame.from_dict({dname: np.mean([percentageConstraint(info.loc[dname]['Labels', pd.NaT],\n",
    "                                                                    assignation.loc[dname][i, method],\n",
    "                                                                    info.loc[dname]['Train', i, method], test = True)\n",
    "                                                                for i in assignation.loc[dname].index.levels[0]])\n",
    "                                                        for dname in measure.index}, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"% train constraint respected\", y=\"% test constraint respected\", sizes=(10, 200), data=measure, \n",
    "                size=info[\"Number of points\"], hue=info[\"Number of clusters\"], palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"% train constraint respected\", y=\"test\", sizes=(10, 200), data=measure, \n",
    "                size=info[\"Number of points\"], hue=info[\"Number of clusters\"], palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On which datasets is it worse ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = scores_average.loc[:, (var, category, metric)].max(axis=\"columns\") # Max\n",
    "difference = difference - scores_average.loc[:, (method, category, metric)] # Remove current score\n",
    "difference = difference[difference > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse = info.loc[difference.index]\n",
    "worse[\"Difference\"] = difference\n",
    "worse[[\"Name\", \"Difference\", \"Number of clusters\", \"Number of points\"]].sort_values(\"Difference\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['24 April 2019 15:28:47']#, '04 April 2019 17:49:23', '02 April 2019 13:09:14', '02 April 2019 13:08:22', '02 April 2019 13:07:48', '07 April 2019 17:09:09', '19 April 2019 12:14:33',  '24 April 2019 15:28:47'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, assignation, constraints, info = openFiles(dates, True, 'outer')\n",
    "print(\"Open {} result files\".format(len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Adjusted Rand\"\n",
    "category = \"test\" # 'all', 'train' or 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.copy().groupby(axis=1, level=[1, 2, 3]).mean()\n",
    "measure = measure.loc[:, (slice(None), category, metric)]\n",
    "measure = measure.T.reset_index(level=[1, 2], drop=True).T\n",
    "\n",
    "deviation = scores.copy().groupby(axis=1, level=[1, 2, 3]).std()\n",
    "deviation = deviation.loc[:, (slice(None), category, metric)]\n",
    "deviation = deviation.T.reset_index(level=[1, 2], drop=True).T\n",
    "\n",
    "length = (~ pd.isna(scores.copy())).groupby(axis=1, level=[1, 2, 3]).sum()\n",
    "length = length.loc[:, (slice(None), category, metric)]\n",
    "length = length.T.reset_index(level=[1, 2], drop=True).T\n",
    "\n",
    "measure['Number of constraints'] = [float(i[i.index('_(') + len('_(True, '): i.rindex(')')]) for i in measure.index]\n",
    "measure['Name'] = info[('Name', pd.NaT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = measure + 1.96 * deviation / np.sqrt(length)\n",
    "lower = measure - 1.96 * deviation / np.sqrt(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in info[('Name', pd.NaT)].unique():\n",
    "    currentDataset = measure[info[('Name', pd.NaT)] == i].sort_values('Number of constraints')\n",
    "    top10 = currentDataset.mean().sort_values(ascending = False)[1:6].index\n",
    "    for method in top10:\n",
    "        plt.plot(currentDataset['Number of constraints'], currentDataset[method])\n",
    "        plt.fill_between(currentDataset['Number of constraints'], lower.loc[currentDataset.index, method], upper.loc[currentDataset.index, method], alpha = 0.1)\n",
    "    plt.title(i)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in info[('Name', pd.NaT)].unique():\n",
    "    currentDataset = measure[info[('Name', pd.NaT)] == i].sort_values('Number of constraints')\n",
    "    currentDataset[currentDataset.mean().sort_values(ascending = False).index].plot(x = 'Number of constraints')\n",
    "    plt.title(i)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference to the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"Bayesian Optimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [v for v in measure.columns if v not in our_methods] + [method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = measure[var].groupby(['Name', 'Number of constraints']).mean()\n",
    "rank = group.rank(axis=1, method = 'min', ascending = False).astype(int)\n",
    "count = (rank == 1).groupby('Number of constraints').sum()\n",
    "a = count.plot.bar(stacked=True)\n",
    "plt.ylabel(\"Number of Datasets\")\n",
    "plt.title(\"Ranked first\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank.groupby('Number of constraints').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
