{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows different visualizations of the algorithms performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open all the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '11 March 2019 12:28:49'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFiles(date): \n",
    "    path = os.path.join(config.result, date)\n",
    "    scores = {}\n",
    "    # Open each dataset\n",
    "    for dname in os.listdir(path):\n",
    "        unpick = pickle.load(open(os.path.join(path, dname), 'rb'))\n",
    "        # Restructure to have a nice pandas dataframe\n",
    "        openScore = {(category, method, metric): unpick[\"Score\"][method][category][metric] \n",
    "                        for method in unpick[\"Score\"] \n",
    "                        for category in unpick[\"Score\"][method]\n",
    "                        for metric in unpick[\"Score\"][method][category]}\n",
    "        openScore.update({(\"Assignation\", method,): unpick[\"Assignation\"][method]\n",
    "                        for method in unpick[\"Assignation\"]})\n",
    "        openScore[(\"Dataset\", \"N_Classes\")] = unpick[\"N_Classes\"]\n",
    "        openScore[(\"Dataset\", \"Constraint\")] = unpick[\"Constraint\"]\n",
    "        openScore[(\"Dataset\", \"Labels\")] = unpick[\"Labels\"]\n",
    "        openScore[(\"Dataset\", \"Train\")] = unpick[\"Train\"]\n",
    "        openScore[(\"Dataset\", \"Name\")] = unpick[\"Name\"]\n",
    "        scores[dname] = openScore\n",
    "    return pd.DataFrame.from_dict(scores, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = openFiles(date)\n",
    "print(\"Open {} result files from {}\".format(len(scores), date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Adjusted Rand\"\n",
    "category = \"test\" # 'all', 'train' or 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the global performances of the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, (category, slice(None), metric)]\n",
    "measure = measure.T.reset_index(level=[0, 2], drop=True).T\n",
    "sns.boxplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure[\"Number classes\"] = scores[(\"Dataset\", \"N_Classes\", )]\n",
    "measure[\"Number points\"] = scores[(\"Dataset\", \"Labels\", )].apply(lambda x: len(x.values[0]), axis = 1) \n",
    "var = [c for c in measure.columns if c not in [\"Number classes\", \"Number points\"]]\n",
    "       \n",
    "cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\n",
    "\n",
    "grid = sns.PairGrid(data = measure, vars = var)\n",
    "grid = grid.map_upper(sns.scatterplot, sizes=(10, 200), size=measure[\"Number points\"], hue=measure[\"Number classes\"], palette = cmap, alpha = 0.7)\n",
    "grid = grid.map_diag(sns.distplot)\n",
    "grid = grid.map_lower(sns.kdeplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and display the rank of the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rank position\n",
    "rank = measure[var].rank(axis = 1, method = 'min',ascending = False).apply(lambda x: x.value_counts()).T \n",
    "rank = rank.sort_values(by=[1, 2, 3], ascending = False) # Reorder\n",
    "rank.plot.barh(stacked=True)\n",
    "plt.xlabel(\"Number dataset\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis by method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"lcvqe\"\n",
    "metric = \"Adjusted Rand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performances between training and test, it is interesting to notice how our method does not overfit where other methods perform well on training but have bad performances on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, ([slice(None)], method, metric)]\n",
    "measure = measure.T.reset_index(level=[1, 2], drop=True).T\n",
    "measure[\"Number points\"] = scores[(\"Dataset\", \"Labels\", )].apply(lambda x: len(x.values[0]), axis = 1) \n",
    "measure[\"Number classes\"] = scores[(\"Dataset\", \"N_Classes\", )]\n",
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"train\", y=\"test\", sizes=(10, 200), data=measure, \n",
    "                size=\"Number points\", hue=\"Number classes\", palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the percentage of cosntraint respected on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from constraint import verification_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageConstraint(gtLabels, label, trainIndices, test = False):\n",
    "    \"\"\"\n",
    "        Compute the percentage of constraint respected\n",
    "        gtLabels -- Ground truth label\n",
    "        label -- Ground truth label\n",
    "        trainIndices -- Indices for train\n",
    "        test -- Compute on test supset if True\n",
    "    \"\"\"\n",
    "    if test:\n",
    "        trainIndices = [i for i in range(len(gtLabels)) if i not in trainIndices]\n",
    "    gtConstraint = 2*np.equal.outer(gtLabels[trainIndices], gtLabels[trainIndices]) - 1\n",
    "    return verification_constraint(gtConstraint, label[trainIndices])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure[\"% train constraint respected\"] = scores.transform(lambda x: percentageConstraint(\n",
    "                                                                    x[('Dataset', 'Labels', pd.NaT)], \n",
    "                                                                    x[('Assignation', method, pd.NaT)], \n",
    "                                                                    x[('Dataset', 'Train', pd.NaT)]),\n",
    "                                                            axis = 1).apply(lambda x: x[0] / np.sum(x))\n",
    "measure[\"% test constraint respected\"] =  scores.transform(lambda x: percentageConstraint(\n",
    "                                                                    x[('Dataset', 'Labels', pd.NaT)], \n",
    "                                                                    x[('Assignation', method, pd.NaT)], \n",
    "                                                                    x[('Dataset', 'Train', pd.NaT)],\n",
    "                                                                    True),\n",
    "                                                            axis = 1).apply(lambda x: x[0] / np.sum(x))\n",
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"% train constraint respected\", y=\"% test constraint respected\", sizes=(10, 200), data=measure, \n",
    "                size=\"Number points\", hue=\"Number classes\", palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"% test constraint respected\", y=\"test\", sizes=(10, 200), data=measure, \n",
    "                size=\"Number points\", hue=\"Number classes\", palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '12 March 2019 16:22:24_evolution'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = openFiles(date)\n",
    "print(\"Open {} result files from {}\".format(len(scores), date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[(\"Dataset\", \"Percentage\", pd.NaT)] = pd.DataFrame.from_dict({p: np.mean(np.abs(scores.loc[p, (\"Dataset\", \"Constraint\", pd.NaT)])) for p in scores.index}, orient = \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Adjusted Rand\"\n",
    "category = \"test\" # 'all', 'train' or 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, (category, slice(None), metric)]\n",
    "measure = measure.T.reset_index(level=[0, 2], drop=True).T\n",
    "measure.groupby(scores[('Dataset', 'Name', pd.NaT)]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
