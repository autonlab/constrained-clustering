{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows different visualizations of the algorithms performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fold import readFold\n",
    "import matplotlib.pyplot as plt\n",
    "from pmlb import classification_dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open all the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all methods are not computed at the same date, indicates several dates in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['18 March 2019 15:55:08', '19 March 2019 15:33:18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromDictToFrame(dictionary, keys = []):\n",
    "    \"\"\"\n",
    "        Transfroms a multidimensional dictionary in a dataframe\n",
    "    \"\"\"\n",
    "    df = {}\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):\n",
    "            df = {**df, **fromDictToFrame(value, keys + [key])}\n",
    "        else:\n",
    "            df[tuple(keys + [key])] = [value]\n",
    "    return pd.DataFrame.from_dict(df)\n",
    "\n",
    "def openFiles(dates):\n",
    "    \"\"\"\n",
    "        Opens all files at the given dates and restructures it in several dataframes for clarity\n",
    "    \"\"\"\n",
    "    scores, assignation, constraints, info = [], [], [], {}\n",
    "    for date in dates:\n",
    "        print(\"Opening {}\".format(date))\n",
    "        path = os.path.join(config.result, date)\n",
    "        scoresDate, assignationDate, constraintsDate = {}, {}, {}\n",
    "        \n",
    "        # Open each dataset\n",
    "        for dname in os.listdir(path):\n",
    "            unpick = pickle.load(open(os.path.join(path, dname), 'rb'))\n",
    "\n",
    "            # Open Score\n",
    "            scoreDname = fromDictToFrame(unpick[\"Score\"]).reset_index()\n",
    "            if not scoreDname.empty:\n",
    "                scoresDate[dname] = scoreDname\n",
    "\n",
    "                # Open Assignation\n",
    "                assignationDate[dname] = fromDictToFrame(unpick[\"Assignation\"])\n",
    "\n",
    "                # Open Constraints\n",
    "                constraintsDate[dname] = fromDictToFrame(unpick[\"Percentage Constraint\"])\n",
    "\n",
    "                # Open info\n",
    "                info[dname] = fromDictToFrame(readFold(unpick[\"Name\"]))\n",
    "\n",
    "        scores.append(pd.concat(scoresDate, axis = \"index\").reset_index(level=[1], drop=True))\n",
    "        assignation.append(pd.concat(assignationDate, axis = \"index\").reset_index(level=[1], drop=True))\n",
    "        constraints.append(pd.concat(constraintsDate, axis = \"index\").reset_index(level=[1], drop=True))\n",
    "\n",
    "    return pd.concat(scores, join = \"inner\", axis = 1),\\\n",
    "        pd.concat(assignation, join = \"inner\", axis = 1),\\\n",
    "        pd.concat(constraints, join = \"inner\", axis = 1),\\\n",
    "        pd.concat(info, axis = \"index\").reset_index(level=[1], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, assignation, constraints, info = openFiles(dates)\n",
    "print(\"Open {} result files\".format(len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Adjusted Rand\"\n",
    "category = \"test\" # 'all', 'train' or 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes mean of the different iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.groupby(axis=1, level=[1, 2, 3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the global performances of the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, (slice(None), category, metric)]\n",
    "measure = measure.T.reset_index(level=[1, 2], drop=True).T\n",
    "sns.boxplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[\"Number points\"] = info['Labels'].applymap(lambda x: len(x)) \n",
    "info[\"Number classes\"] = info[\"N_Classes\"]\n",
    "cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\n",
    "\n",
    "grid = sns.PairGrid(data = measure)\n",
    "grid = grid.map_upper(sns.scatterplot, sizes=(10, 200), size=info[\"Number points\"], hue=info[\"Number classes\"], palette = cmap, alpha = 0.7)\n",
    "grid = grid.map_diag(sns.distplot)\n",
    "grid = grid.map_lower(sns.kdeplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and display the rank of the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [v for v in measure.columns if v != \"Bayesian Optimization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rank position\n",
    "rank = measure[var].rank(axis = 1, method = 'min',ascending = False).apply(lambda x: x.value_counts()).T \n",
    "rank = rank.sort_values(by=[1, 2, 3], ascending = False) # Reorder\n",
    "rank.plot.barh(stacked=True)\n",
    "plt.xlabel(\"Number dataset\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis by method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"lcvqe\"\n",
    "metric = \"Adjusted Rand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performances between training and test, it is interesting to notice how our method does not overfit where other methods perform well on training but have bad performances on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, (method, slice(None), metric)]\n",
    "measure = measure.T.reset_index(level=[0, 2], drop=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"train\", y=\"test\", sizes=(10, 200), data=measure, \n",
    "                size=info[\"Number points\"], hue=info[\"Number classes\"], palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the percentage of cosntraint respected on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constraint import verification_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageConstraint(gtLabels, label, trainIndices, test = False):\n",
    "    \"\"\"\n",
    "        Compute the percentage of constraint respected\n",
    "        gtLabels -- Ground truth label\n",
    "        label -- Ground truth label\n",
    "        trainIndices -- Indices for train\n",
    "        test -- Compute on test supset if True\n",
    "    \"\"\"\n",
    "    if test:\n",
    "        trainIndices = [i for i in range(len(gtLabels)) if i not in trainIndices]\n",
    "    gtConstraint = 2*np.equal.outer(gtLabels[trainIndices], gtLabels[trainIndices]) - 1\n",
    "    \n",
    "    verified, notVerified = verification_constraint(gtConstraint, label[trainIndices])\n",
    "    return  verified / (notVerified + verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure[\"% train constraint respected\"] = pd.DataFrame.from_dict({dname: np.mean([percentageConstraint(info.loc[dname]['Labels', pd.NaT],\n",
    "                                                                    assignation.loc[dname][i, method],\n",
    "                                                                    info.loc[dname]['Train', i, method])\n",
    "                                                                for i in assignation.loc[dname].index.levels[0]])\n",
    "                                                        for dname in measure.index}, orient = 'index')\n",
    "\n",
    "measure[\"% test constraint respected\"] = pd.DataFrame.from_dict({dname: np.mean([percentageConstraint(info.loc[dname]['Labels', pd.NaT],\n",
    "                                                                    assignation.loc[dname][i, method],\n",
    "                                                                    info.loc[dname]['Train', i, method], test = True)\n",
    "                                                                for i in assignation.loc[dname].index.levels[0]])\n",
    "                                                        for dname in measure.index}, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"% train constraint respected\", y=\"% test constraint respected\", sizes=(10, 200), data=measure, \n",
    "                size=info[\"Number points\"], hue=info[\"Number classes\"], palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(method)\n",
    "plt.grid(alpha = 0.1)\n",
    "sns.scatterplot(x=\"% train constraint respected\", y=\"test\", sizes=(10, 200), data=measure, \n",
    "                size=info[\"Number points\"], hue=info[\"Number classes\"], palette = cmap, alpha = 0.7)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['18 March 2019 16:10:35']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, assignation, constraints, info = openFiles(dates)\n",
    "print(\"Open {} result files\".format(len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Adjusted Rand\"\n",
    "category = \"test\" # 'all', 'train' or 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, (slice(None), slice(None), category, metric)].stack(level = 0)\n",
    "measure['Percentage Constraint'] = constraints.stack().max(axis = 1)\n",
    "measure = measure.T.reset_index(level=[1, 2], drop=True).T\n",
    "measure = measure.reset_index(level=[1], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in info[('Name', pd.NaT)].unique():\n",
    "    currentDataset = measure[info[('Name', pd.NaT)] == i]\n",
    "    currentDataset[\"Percentage Constraint\"] = currentDataset[\"Percentage Constraint\"].apply(lambda x: round(x, 2))\n",
    "    currentDataset = currentDataset.groupby(\"Percentage Constraint\").mean().sort_index()\n",
    "    currentDataset.plot()\n",
    "    plt.title(i)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
