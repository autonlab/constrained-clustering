{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows different visualizations of the algorithms performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open all the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(config.result, date)\n",
    "scores = {}\n",
    "for dname in os.listdir(path):\n",
    "    unpick = pickle.load(open(os.path.join(path, dname), 'rb'))\n",
    "    openScore = {(category, method, metric): unpick[\"Score\"][method][category][metric] \n",
    "                    for method in unpick[\"Score\"] \n",
    "                    for category in unpick[\"Score\"][method]\n",
    "                    for metric in unpick[\"Score\"][method][category]}\n",
    "    openScore[(\"Dataset\", \"N_Classes\")] = unpick[\"N_Classes\"]\n",
    "    openScore[(\"Dataset\", \"Constraint\")] = unpick[\"Constraint\"]\n",
    "    openScore[(\"Dataset\", \"Labels\")] = unpick[\"Labels\"]\n",
    "    openScore[(\"Dataset\", \"Train\")] = unpick[\"Train\"]\n",
    "    scores[unpick['Name']] = openScore\n",
    "print(\"Open {} result files from {}\".format(len(scores), date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame.from_dict(scores, orient=\"index\")\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"Adjusted Rand\"\n",
    "category = \"test\" # 'all', 'train' or 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, (category, slice(None), metric)]\n",
    "measure = measure.T.reset_index(level=[0, 2], drop=True).T\n",
    "sns.boxplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data = measure, orient=\"h\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.xlabel(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display which method has the best score on every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rank position\n",
    "rank = measure.rank(axis = 1, method = 'min',ascending = False).apply(lambda x: x.value_counts()).T \n",
    "rank = rank.sort_values(by=[1, 2, 3], ascending = False) # Reorder\n",
    "rank.plot.barh(stacked=True)\n",
    "plt.xlabel(\"Number dataset\")\n",
    "plt.ylabel(\"Methods\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Baysian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"mpckm\"\n",
    "metric = \"Adjusted Rand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = scores.loc[:, ([slice(None)], method, metric)]\n",
    "measure = measure.T.reset_index(level=[1, 2], drop=True).T\n",
    "measure[\"Number points\"] = scores[(\"Dataset\", \"Labels\", )].apply(lambda x: len(x.values[0]), axis = 1) \n",
    "measure[\"Number classes\"] = scores[(\"Dataset\", \"N_Classes\", )]\n",
    "cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\n",
    "plt.title(method)\n",
    "sns.scatterplot(x=\"train\", y=\"test\", sizes=(10, 200), data=measure, \n",
    "                size=\"Number points\", hue=\"Number classes\", palette = cmap, alpha = 0.7)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
